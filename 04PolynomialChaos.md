# Orthogonal basis and Polynomial Chaos

## Mathematical Background

Let $x$ be a random variable with support $\mathcal X$. As a first instance $x$ will be one dimensional, but it will be showed later that this hypothesis doens't limit our treatment of multiple input systems. Let $w: \mathcal X \rightarrow \mathbb R$ be the probability distribution of $x$. If $f(x)$ and $g(x)$ are two functions of $x$ we can define their inner product as:

:::{math}
:label: innerProduct
 \langle f(x), g(x)\rangle_w = \int_{\mathcal X}f(x)g(x)w(x)dx
:::

Thus it is possible to define the notion of "orthogonality" between functions in the same way as we would for vectors: two functions $f$ and $g$ are orthogonal if and only if their inner product is 0, or:

:::{math}
 \langle f(x), g(x)\rangle_w = \int_{\mathcal X}f(x)g(x)w(x)dx = 0
:::

As stated in {cite}`augustin2008polynomial`, if a function $f(x)$ has finite variance ($\langle f(x), f(x)\rangle_w < \infty$) then we "expand" it in a series of orthogonal polynomials of $x$ as:

:::{math}
	f(x) = \sum_{i=0}^{\infty} \beta_i\Phi_i(x)
:::

where $\beta_i$ are real coefficients and ${\Phi_i(x)}_ {i\in\mathbb N}$ is the basis of orthogonal polynomials of $x$. Such an expansion can be truncated to an order $d$, thus obtaining an approximation of the function $f(x)$ in a set of orthogonal polynomials in a similar fashion as what is commonly done with Fourier series.

## How to compute the coefficients

Following {cite}`oladyshkin2012data` it can be noted that the way the conditions are set for the constructions of the polynomials leads to a way of determining them all in one time as the solution of a linear system of equations. 

The conditions to uniquely determine a polynomial are:
* The leading term must be 1.
* Every polynomial of degree $n$ must be orthogonal to all those of lower degree.

Calling a polynomial of this king of degree $n$ as $\pi_n(x)$ (where $x\in \mathcal{X}$ is a random variable with probability distribution $w$), it follows immediately that $\pi_0(x) = 1$.\
The condition of orthogonality, defined as:

$$ \langle \pi_i,\pi_j\rangle_w = \int_{\mathcal{X}} \pi_i(x)\pi_j(x)w(x)dx = 0\quad\mbox{for }i\neq j $$ (eqn:orthogonality)

results in, for the first polynomial ($\pi_1(x) = c_1^1x + c_1^0$):

$$
\begin{gather}
	\langle \pi_1,\pi_0\rangle_w = \int_{\mathcal{X}} \pi_1(x)\cdot1\cdot w(x)dx = 0\\
	c_1^1 = 1
\end{gather}
$$

which results in:

$$
\begin{gather}
	\int_{\mathcal{X}} \pi_1(x)\cdot1\cdot w(x)dx = 0 = c_1^1\mu_1 + c_1^0\mu_0 \\
	c_1^1 = 1
\end{gather}
$$

where, from now on, $\mu_i$ is the statistical moment of order $i$ ($\mu_i = \int_{\mathcal{X}}x^iw(x)dx$), so that $\mu_o = 1$. It is interesting to notice that, when $\mu_1 = 0$, we obtain $\pi_1(x) = x$. This form for $\pi_1$ is sometimes assumed, but without any loss of genereality since simply subtracting the mean to any distribution will render this result correct anyway.

The second polynomial, *i.e.* $\pi_2(x) = c_2^2x^2 + c_2^1x + c_2^0$, will then follow:

$$
\begin{gather}
	\int_{\mathcal{X}} \pi_2(x)\cdot1\cdot w(x)dx = 0 \\
	\int_{\mathcal{X}} \pi_2(x)\pi_1(x)w(x)dx = 0\\
	c_2^2 = 1
\end{gather}
$$

The known form for $\pi_1$ we arrive at the following system:

$$
\begin{gather}
	\mu_0c_2^0 + \mu_1c_2^1 + \mu_2c_2^2 = 0\\
	\mu_1c_2^0 + \mu_2c_2^1 + \mu_3c_2^2 = 0\\
	c_2^2 = 1
\end{gather}
$$

Or, in matrix form:

$$
\begin{bmatrix} 
	\mu_0&\mu_1&\mu_2\\
	\mu_1 & \mu_2 & \mu_3\\
	0 & 0 & 1
\end{bmatrix}\begin{bmatrix} c_2^0\\c_2^1\\c_2^2\end{bmatrix} = \begin{bmatrix} 0\\0\\1\end{bmatrix}
$$

Since the moments of the distribution can be easily calculated from the data generated by the distribution, the only thing left to compute all the coefficients is to invert the matrix. This same methodology can be applied tp polynomials of any degree, yielding for the polynomial $\pi_n = \sum_{i=0}^n c_n^ix^i$:

$$
\begin{bmatrix}
\mu_0 & \mu_1 & \dots & \mu_n\\
\mu_1 & \mu_2 & \dots & \mu_{n+1}\\
\vdots & \vdots & \ddots & \vdots\\
\mu_{n-1} & \mu_n & \dots & \mu_{2n-1}\\
0 & 0 & \dots & 1
\end{bmatrix}\begin{bmatrix} c_n^0\\c_n^1\\\vdots\\c_n^{n-1}\\c_n^n\end{bmatrix} = \begin{bmatrix} 0\\0\\\vdots\\1\end{bmatrix}
$$(eqn:moments-matrix)

This shows that there is no need to integrate any function numerically and that only the knowledge of the first $2n-1$ moments of the distribution of the data is necessary in order to calculate all the coefficients of the expansion up to degree $n$ by inverting the moment matrix $M$.

In case we desire the basis to be ortho*normal* we can simply divide the coefficients of the polynomial $\pi_i$ by its norm, which is defined as:

:::{math}
:label: norm_polynomial
\| \pi_i\|_ w^2 = \int_{\mathcal{X}} \pi_i^2(x)w(x)dx 
:::

Thus, from {eq}`norm_polynomial`, for calculating the norm of $\pi_i$ it is necessary to know the moments up to $\mu_{2i}$. Thus in the orthonormal expansion of degree $d$ we need to compute the moments up to $\mu_{2d}$

A comment is due to the invertibility of the matrix of the moments. As stated in {cite}`oladyshkin2012data` if $\mbox{rank}(M) = k$ then $\mbox{det}(M) = 0$ if and only if the cardinality of $\mathcal X$ is $\leq k$. This, as it will be shown in <span style="color: red;">section</span>, limits the form that the input to the system may take, but assures that the numerical method can be carried out without problems if this condition is respected, which is the case for most real case scenarios.

## Multivariate Polynomial Chaos

The previous section only deals with single valued random variables. But what happens when the input is multidimensional?

Let the input be $X = [x_0, x_1, \dots, x_N]\in\mathcal X ^N$. Under the assumption that all $x_i$ are independent, the resulting multivariate polynomial expansion of maximum degree $d$ is just the multiplication of the single variable ones:

$$
\begin{gather}
	Y(x) = \sum_i \beta_i\Phi_i(X)\\
	\Phi_i(X) = \prod_{j= 0}^N \pi_{\alpha^i_j}^j(X)\\
	\mbox{with: }\sum_{j=0}^N \alpha^i_j \leq d
\end{gather}
$$

where the index matrix $\alpha^i_j$ contains all the combinatorial information needed to accurately multiply the right coefficients. It must be noted that now the number of terms in the expansion is increased to $M = (N+d)!/(N!d!)$ to accomodate for all the possible combinations of order less than $d$.

The matrix can be constructed in the following way. We let every column represent a variable and every row a term in the expansion. In this way the entire matrix will be of dimension $d\times N$. Every entry $\alpha^i_j$ will represent the degree of the polynomial of the variable $j$ in the term $i$. An example will hopefully clarify this idea.

Let $X = [x_0, x_1]$ and the maximum degree of the expansion be $M = 2$. Let the polynomial of degree $i$ of the variable $x_j$ be called $\pi_i^j$. This will generate an expansion with $M = 6$ terms, in the form:

$$
	Y(X) = \beta_0\pi_0^0\pi_0^1 + \beta_1\pi_1^0\pi_0^1 + \beta_2\pi_0^0\pi_1^1 + \beta_3\pi_2^0\pi_0^1 + \beta_4\pi_1^0\pi_1^1 + \beta_5\pi_0^0\pi_2^1
$$

The alpha matrix that indicates such a combination would thus be:

$$
\alpha = \begin{bmatrix}
	0&0\\1&0\\0&1\\2&0\\1&1\\0&2
\end{bmatrix}
$$  

